jay722.lee@test-tgi:/group-volume/jay722.lee/github/text-generation-inference$ python ./server/text_generation_server/cli.py serve heegyu/kogpt-j-base
Traceback (most recent call last):

  File "/group-volume/jay722.lee/github/text-generation-inference/./server/text_generation_server/cli.py", line 187, in <module>
    app()

  File "/group-volume/jay722.lee/github/text-generation-inference/./server/text_generation_server/cli.py", line 58, in serve
    from text_generation_server import server

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/server.py", line 12, in <module>
    from text_generation_server.cache import Cache

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/cache.py", line 3, in <module>
    from text_generation_server.models.types import Batch

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/__init__.py", line 9, in <module>
    from text_generation_server.models.model import Model

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/model.py", line 8, in <module>
    from text_generation_server.models.types import Batch, GeneratedText

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/types.py", line 9, in <module>
    from text_generation_server.pb import generate_pb2

ImportError: cannot import name 'generate_pb2' from 'text_generation_server.pb' (unknown location)




ImportError: cannot import name 'generate_pb2' from 'text_generation_server.pb' (unknown location)

text-generation-inference/server> make install

-----------------------------------------------------------------------------



jay722.lee@test-tgi:/group-volume/jay722.lee/github/text-generation-inference/server$ python ./text_generation_server/cli.py serve heegyu/kogpt-j-base

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so
/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//s3.n6.sr-cloud.com')}
  warn(msg)
/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Seoul')}
  warn(msg)
/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//75.17.107.42'), PosixPath('http'), PosixPath('8080')}
  warn(msg)
/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8000'), PosixPath('//kong.mlp')}
  warn(msg)
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116.so...
triton not installed.
We're not using custom kernels.
Traceback (most recent call last):

  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 187, in <module>
    app()

  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 58, in serve
    from text_generation_server import server

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/server.py", line 12, in <module>
    from text_generation_server.cache import Cache

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/cache.py", line 3, in <module>
    from text_generation_server.models.types import Batch

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/__init__.py", line 20, in <module>
    from text_generation_server.models.gptj import GPTJ, GPTJSharded

  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/gptj.py", line 13, in <module>
    from transformers.models.gptj.parallel_layers import (

ModuleNotFoundError: No module named 'transformers.models.gptj.parallel_layers'

pip uninstall transformers
pip install -e .

------------------------------------------------------

> File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/__init__.py", line 74, in <module>
    from text_generation_server.models.flash_rw import FlashRWSharded
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/flash_rw.py", line 9, in <module>
    from text_generation_server.models.custom_modeling.flash_rw_modeling import (
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/custom_modeling/flash_rw_modeling.py", line 10, in <module>
    import flash_attn_cuda
ModuleNotFoundError: No module named 'flash_attn_cuda'
Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 239kB/s]
Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 790/790 [00:00<00:00, 255kB/s]
Downloading (…)olve/main/vocab.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.27M/1.27M [00:00<00:00, 1.70MB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 925k/925k [00:00<00:00, 1.20MB/s]
Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.07M/3.07M [00:01<00:00, 2.34MB/s]
Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96.0/96.0 [00:00<00:00, 114kB/s]
Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 667M/667M [02:25<00:00, 4.58MB/s]
Floating point exception (core dumped)


make install-flash-attention
or 
export USE_FLASH_ATTENTION=false

------------------------------------------------------

text-generation-server download-weights t5-base


Error when initializing model
Traceback (most recent call last):
  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 187, in <module>
    app()
  File "/usr/local/lib/python3.9/dist-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/typer/core.py", line 778, in main
    return _main(
  File "/usr/local/lib/python3.9/dist-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/usr/local/lib/python3.9/dist-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/usr/local/lib/python3.9/dist-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/usr/local/lib/python3.9/dist-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 67, in serve
    server.serve(model_id, revision, sharded, quantize, trust_remote_code, uds_path)
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/server.py", line 155, in serve
    asyncio.run(serve_inner(model_id, revision, sharded, quantize, trust_remote_code))
  File "/usr/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.9/asyncio/base_events.py", line 629, in run_until_complete
    self.run_forever()
  File "/usr/lib/python3.9/asyncio/base_events.py", line 596, in run_forever
    self._run_once()
  File "/usr/lib/python3.9/asyncio/base_events.py", line 1890, in _run_once
    handle._run()
  File "/usr/lib/python3.9/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
> File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/server.py", line 124, in serve_inner
    model = get_model(model_id, revision, sharded, quantize, trust_remote_code)
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/__init__.py", line 257, in get_model
    return T5Sharded(
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/t5.py", line 60, in __init__
    model = T5ForConditionalGeneration(config, weights)
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/models/custom_modeling/t5_modeling.py", line 1031, in __init__
    self.lm_head = TensorParallelHead.load(
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/utils/layers.py", line 171, in load
    weight = weights.get_sharded(f"{prefix}.weight", dim=0)
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/utils/weights.py", line 66, in get_sharded
    filename, tensor_name = self.get_filename(tensor_name)
  File "/group-volume/jay722.lee/github/text-generation-inference/server/text_generation_server/utils/weights.py", line 42, in get_filename
    raise RuntimeError(f"weight {tensor_name} does not exist")
RuntimeError: weight lm_head.weight does not exist

---------------------------------------------------------------------------

text-generation-server download-weights bigscience/bloom-560m


---------------------------------------------------------------------


jay722.lee@test-tgi:/group-volume/jay722.lee/github/text-generation-inference/server$ python ./text_generation_server/cli.py serve heegyu/kogpt-j-base --sharded
Traceback (most recent call last):

  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 187, in <module>
    app()

  File "/group-volume/jay722.lee/github/text-generation-inference/server/./text_generation_server/cli.py", line 32, in serve
    assert (

AssertionError: RANK must be set when sharded is True

>>>>
SAFETENSORS_FAST_GPU=1 python -m torch.distributed.run --nproc_per_node=2 text_generation_server/cli.py serve bigscience/bloom-560m --sharded

----------------------------------------------------------------------------